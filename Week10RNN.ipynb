{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdindaRahajengSilviaPranesti/MarchineLearning_2141720158_AdindaRSP_02/blob/main/Week10RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wG-ze4C-AVbU"
      },
      "source": [
        "#ðŸ–¥ **Job Sheet 10: RNN**\n",
        "####- **ðŸŒ¼Adinda Rahajeng Silvia PranestiðŸŒ¼**\n",
        "###- **ðŸŒ¼2141720158 / 02**\n",
        "###- **ðŸŒ¼3I-TI**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwBeA0EOBmx_"
      },
      "source": [
        "##**ðŸ–¥LAB WORK 1**\n",
        "**RNN for Sentiment Analysis**\n",
        "\n",
        "**Setup**\n",
        "\n",
        "Import matplotlib and create a helper function to plot the graph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NVaibMVfChqB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "tfds.disable_progress_bar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cDWt9qh4CwM-"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history.history['val_'+metric], '')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend([metric, 'val_'+metric])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkH1aEKFC-1v"
      },
      "source": [
        "**Setup input pipeline**\n",
        "\n",
        "The IMDB movie review dataset is a binary classification datasetâ€”all reviews have either positive or negative sentiment.\n",
        "Download the dataset using TFDS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5-KhiOmDBSw",
        "outputId": "11fd5f8a-9612-49c6-8d4a-f43a77963239"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\n",
            "Dataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
              " TensorSpec(shape=(), dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "dataset, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
        "train_dataset.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XTvIzT9_wji",
        "outputId": "c4078f31-010c-41f0-f1e9-657c7df6ccfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text:  b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n",
            "label:  0\n"
          ]
        }
      ],
      "source": [
        "# Initially, this returns a dataset (text, label pairs):\n",
        "for example, label in train_dataset.take(1):\n",
        "  print('text: ', example.numpy())\n",
        "  print('label: ', label.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2z4exqE8E6Hs",
        "outputId": "cca6fd84-5b77-4003-b02a-528648624c7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "texts:  [b'This started bad, got worse, and by the time the girl attacked the old lady at the end i literally wanted to take the DVD to the person we borrowed it off and choke the C**T to death with it. Avoid this film, a little bit of good cinematography and some naked shots, would be almost acceptable if i was 14 and had not seen Jenna Jameson naked a million times. If anyone feels the need to watch this film, i would strongly recommend you spend the time more appropriately, as an example i would say trying to cram a Lego house into your bum with no lube would be a good start. I hear that this film was not the original version, i would very much like to view the original, as it seems that this cut version is devoid of all plot, and apparently most of the nudity, can someone please tell me how i can get in touch with Christian Viel he owes me an hour of my life back!'\n",
            " b'In light of the recent and quite good Batman the Brave and the Bold, now is the time to bear a fatal blow to that mistake in the life of Batman. Being a huge fan since the first revival by Tim Burton 20 years ago, I have been able to accept different tonalities in the character, dark or campy. This one is just not credible : too many effects, poor intrigues and so few questions. What is great about Batman is the diversity of his skills and aspects of his personality : detective, crime-fighter, playboy, philanthropist etc. The Batman shows him only in his karate days. And by the way, how come the Penguin is capable of such virtuosity when jumping in the air regardless of his portly corpulence ? And look at the Joker, a mixture of Blanka in Street Fighter 2 and a stereotypical reggae man, what Batman fan could accept such a treason ? Not me anyway. Batman is much better without \"The\" article in front of his name.'\n",
            " b\"Stewart Kane (Gabriel Byrne, VANITY FAIR) heads out with his local Jindabyne, Australia fishing buddies for a weekend of rest, recreation, and relaxation. But when Stewart discovers an aboriginal woman's body floating face-down in a river, things appear to have turned out for the worst. The largest casualty of the weekend is the men's commonsense. They don't hike out of the ravine, and instead finish their fishing weekend with some great catches. Then they head out and report the body.<br /><br />The town and the men's lives quickly turn into a mess. The local media swarms them, and accusations of aboriginal prejudices rear up from the local natives. Stewart's wife Claire (Laura Linney, THE EXORCISM OF EMILY ROSE) senses the deeper meanings of what her husband and his friends did, but has to battle with it through her own mental illness.<br /><br />Amidst all this chaos is the life that was this young woman who is now a media spectacle, splayed out on a morgue slab. Her murder and subsequent dumping into the water are symbolic of what lay beneath the town of Jindabyne: a division of men and women, black and white, social and outcast.<br /><br />The only other people who seem to understand some of what is going on are two young kids: Stewart and Claire's son who is being led around by a half-breed Aussie who's mother was killed also just a few years before. The young girl lives with her grandparents and is trying to let go of her mother the best way she can, and the discovery of a new body seems \\xc2\\x97 strangely enough \\xc2\\x97 a method in which to accomplish this (again, the underlying current of Jindabyne is surmised).<br /><br />Everything and everyone in this Jindabyne township feels what lurks beneath its surface, yet none of them are willing to dive into the murky waters and take a look around (the symbolism here is seen when a nearby lake that is used for recreation and swimming is said to contain the old town of Jindabyne under its surface). None, that is, until Claire forces them to.<br /><br />The movie is interesting if a bit too convoluted. There are far too many story lines that needed exploring and it just doesn't get done; too many loose threads. The acting was okay, but the filming was terrible. Wobbly cameras, grainy or dark shots, and just a generalized sloppiness hurt the overall production.<br /><br />I enjoy symbolic films, NORTHFORK being one of my all-time favorites in that vein. But Jindabyne needed to peak its head above the turbid water so that it could see its own problems, which simply didn't happen.\"]\n",
            "\n",
            "labels:  [0 0 0]\n"
          ]
        }
      ],
      "source": [
        "# Next, shuffle the data for training and create this pair dataset:\n",
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "for example, label in train_dataset.take(1):\n",
        "  print('texts: ', example.numpy()[:3])\n",
        "  print()\n",
        "  print('labels: ', label.numpy()[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NarZO5LyFTeT"
      },
      "source": [
        "**Create a Text Encoder**\n",
        "\n",
        "\n",
        "Raw text loaded by tfds needs to be processed before it can be used in a model. The easiest way to preprocess text for training is to use the TextVectorization layer. This layer has many capabilities, but in this tutorial, we are using the default behavior. Create this layer and pass the text dataset to the layer's .adapt method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HRL5Rg0bFVkP"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 1000\n",
        "encoder = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE)\n",
        "encoder.adapt(train_dataset.map(lambda text, label: text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVlS0-eWFgkB",
        "outputId": "7674d6ab-d59d-407f-d55a-0585357e663d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i',\n",
              "       'this', 'that', 'br', 'was', 'as', 'for', 'with', 'movie', 'but'],\n",
              "      dtype='<U14')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# After padding and unknown tokens, they are sorted by frequency:\n",
        "vocab = np.array(encoder.get_vocabulary())\n",
        "vocab[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrSy1CqQFhoZ",
        "outputId": "3347330d-638d-4079-b228-164a06fda8a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 11, 628,  84, ...,   0,   0,   0],\n",
              "       [  8, 705,   5, ...,   0,   0,   0],\n",
              "       [  1,   1,   1, ..., 316, 153, 596]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# The index tensor is padded with 0s to the longest sequence in the batch (unless if you set output_sequence_length fixed):\n",
        "encoded_example = encoder(example)[:3].numpy()\n",
        "encoded_example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOR4fW1eGZoj",
        "outputId": "fc0fc507-6e5d-4f30-a3bc-db98d077bba1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  b'This started bad, got worse, and by the time the girl attacked the old lady at the end i literally wanted to take the DVD to the person we borrowed it off and choke the C**T to death with it. Avoid this film, a little bit of good cinematography and some naked shots, would be almost acceptable if i was 14 and had not seen Jenna Jameson naked a million times. If anyone feels the need to watch this film, i would strongly recommend you spend the time more appropriately, as an example i would say trying to cram a Lego house into your bum with no lube would be a good start. I hear that this film was not the original version, i would very much like to view the original, as it seems that this cut version is devoid of all plot, and apparently most of the nudity, can someone please tell me how i can get in touch with Christian Viel he owes me an hour of my life back!'\n",
            "Round-trip:  this started bad got worse and by the time the girl [UNK] the old lady at the end i [UNK] wanted to take the dvd to the person we [UNK] it off and [UNK] the [UNK] to death with it avoid this film a little bit of good cinematography and some [UNK] shots would be almost [UNK] if i was [UNK] and had not seen [UNK] [UNK] [UNK] a [UNK] times if anyone feels the need to watch this film i would [UNK] recommend you [UNK] the time more [UNK] as an example i would say trying to [UNK] a [UNK] house into your [UNK] with no [UNK] would be a good start i hear that this film was not the original version i would very much like to view the original as it seems that this cut version is [UNK] of all plot and apparently most of the [UNK] can someone please tell me how i can get in [UNK] with [UNK] [UNK] he [UNK] me an hour of my life back                                                                                                                                                                                                                                                                                \n",
            "\n",
            "Original:  b'In light of the recent and quite good Batman the Brave and the Bold, now is the time to bear a fatal blow to that mistake in the life of Batman. Being a huge fan since the first revival by Tim Burton 20 years ago, I have been able to accept different tonalities in the character, dark or campy. This one is just not credible : too many effects, poor intrigues and so few questions. What is great about Batman is the diversity of his skills and aspects of his personality : detective, crime-fighter, playboy, philanthropist etc. The Batman shows him only in his karate days. And by the way, how come the Penguin is capable of such virtuosity when jumping in the air regardless of his portly corpulence ? And look at the Joker, a mixture of Blanka in Street Fighter 2 and a stereotypical reggae man, what Batman fan could accept such a treason ? Not me anyway. Batman is much better without \"The\" article in front of his name.'\n",
            "Round-trip:  in light of the [UNK] and quite good [UNK] the [UNK] and the [UNK] now is the time to [UNK] a [UNK] [UNK] to that [UNK] in the life of [UNK] being a huge fan since the first [UNK] by [UNK] [UNK] 20 years ago i have been able to [UNK] different [UNK] in the character dark or [UNK] this one is just not [UNK] too many effects poor [UNK] and so few [UNK] what is great about [UNK] is the [UNK] of his [UNK] and [UNK] of his [UNK] [UNK] [UNK] [UNK] [UNK] etc the [UNK] shows him only in his [UNK] days and by the way how come the [UNK] is [UNK] of such [UNK] when [UNK] in the air [UNK] of his [UNK] [UNK] and look at the [UNK] a [UNK] of [UNK] in street [UNK] 2 and a [UNK] [UNK] man what [UNK] fan could [UNK] such a [UNK] not me anyway [UNK] is much better without the [UNK] in [UNK] of his name                                                                                                                                                                                                                                                                                     \n",
            "\n",
            "Original:  b\"Stewart Kane (Gabriel Byrne, VANITY FAIR) heads out with his local Jindabyne, Australia fishing buddies for a weekend of rest, recreation, and relaxation. But when Stewart discovers an aboriginal woman's body floating face-down in a river, things appear to have turned out for the worst. The largest casualty of the weekend is the men's commonsense. They don't hike out of the ravine, and instead finish their fishing weekend with some great catches. Then they head out and report the body.<br /><br />The town and the men's lives quickly turn into a mess. The local media swarms them, and accusations of aboriginal prejudices rear up from the local natives. Stewart's wife Claire (Laura Linney, THE EXORCISM OF EMILY ROSE) senses the deeper meanings of what her husband and his friends did, but has to battle with it through her own mental illness.<br /><br />Amidst all this chaos is the life that was this young woman who is now a media spectacle, splayed out on a morgue slab. Her murder and subsequent dumping into the water are symbolic of what lay beneath the town of Jindabyne: a division of men and women, black and white, social and outcast.<br /><br />The only other people who seem to understand some of what is going on are two young kids: Stewart and Claire's son who is being led around by a half-breed Aussie who's mother was killed also just a few years before. The young girl lives with her grandparents and is trying to let go of her mother the best way she can, and the discovery of a new body seems \\xc2\\x97 strangely enough \\xc2\\x97 a method in which to accomplish this (again, the underlying current of Jindabyne is surmised).<br /><br />Everything and everyone in this Jindabyne township feels what lurks beneath its surface, yet none of them are willing to dive into the murky waters and take a look around (the symbolism here is seen when a nearby lake that is used for recreation and swimming is said to contain the old town of Jindabyne under its surface). None, that is, until Claire forces them to.<br /><br />The movie is interesting if a bit too convoluted. There are far too many story lines that needed exploring and it just doesn't get done; too many loose threads. The acting was okay, but the filming was terrible. Wobbly cameras, grainy or dark shots, and just a generalized sloppiness hurt the overall production.<br /><br />I enjoy symbolic films, NORTHFORK being one of my all-time favorites in that vein. But Jindabyne needed to peak its head above the turbid water so that it could see its own problems, which simply didn't happen.\"\n",
            "Round-trip:  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] out with his local [UNK] [UNK] [UNK] [UNK] for a [UNK] of rest [UNK] and [UNK] but when [UNK] [UNK] an [UNK] [UNK] body [UNK] [UNK] in a [UNK] things appear to have turned out for the worst the [UNK] [UNK] of the [UNK] is the [UNK] [UNK] they dont [UNK] out of the [UNK] and instead [UNK] their [UNK] [UNK] with some great [UNK] then they head out and [UNK] the [UNK] br the town and the [UNK] lives quickly turn into a mess the local [UNK] [UNK] them and [UNK] of [UNK] [UNK] [UNK] up from the local [UNK] [UNK] wife [UNK] [UNK] [UNK] the [UNK] of [UNK] [UNK] [UNK] the [UNK] [UNK] of what her husband and his friends did but has to battle with it through her own [UNK] [UNK] br [UNK] all this [UNK] is the life that was this young woman who is now a [UNK] [UNK] [UNK] out on a [UNK] [UNK] her murder and [UNK] [UNK] into the [UNK] are [UNK] of what [UNK] [UNK] the town of [UNK] a [UNK] of men and women black and white [UNK] and [UNK] br the only other people who seem to understand some of what is going on are two young kids [UNK] and [UNK] son who is being [UNK] around by a [UNK] [UNK] whos mother was killed also just a few years before the young girl lives with her [UNK] and is trying to let go of her mother the best way she can and the [UNK] of a new body seems [UNK] [UNK] enough [UNK] a [UNK] in which to [UNK] this again the [UNK] [UNK] of [UNK] is [UNK] br everything and everyone in this [UNK] [UNK] feels what [UNK] [UNK] its [UNK] yet none of them are [UNK] to [UNK] into the [UNK] [UNK] and take a look around the [UNK] here is seen when a [UNK] [UNK] that is used for [UNK] and [UNK] is said to [UNK] the old town of [UNK] under its [UNK] none that is until [UNK] [UNK] them [UNK] br the movie is interesting if a bit too [UNK] there are far too many story lines that needed [UNK] and it just doesnt get done too many [UNK] [UNK] the acting was okay but the [UNK] was terrible [UNK] [UNK] [UNK] or dark shots and just a [UNK] [UNK] [UNK] the overall [UNK] br i enjoy [UNK] films [UNK] being one of my [UNK] [UNK] in that [UNK] but [UNK] needed to [UNK] its head above the [UNK] [UNK] so that it could see its own problems which simply didnt happen\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for n in range(3):\n",
        "  print(\"Original: \", example[n].numpy())\n",
        "  print(\"Round-trip: \", \" \".join(vocab[encoded_example[n]]))\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Qj-BIe_GhcA"
      },
      "source": [
        "##**Create a Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VLElqLjCGo1_"
      },
      "outputs": [],
      "source": [
        "#STEP 1\n",
        "model = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=len(encoder.get_vocabulary()),\n",
        "        output_dim=64,\n",
        "        # Use masking to handle the variable sequence lengths\n",
        "        mask_zero=True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLWonAEYGyh3",
        "outputId": "abdccce0-a00f-4305-cf5b-a572b04e692d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[False, True, True, True, True]\n"
          ]
        }
      ],
      "source": [
        "# step 2\n",
        "print([layer.supports_masking for layer in model.layers])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGF_qeoRG7_A",
        "outputId": "64ae7bf4-67ff-42ee-fa7f-dc590fb52d77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 3s 3s/step\n",
            "[0.01290771]\n"
          ]
        }
      ],
      "source": [
        "# step 3\n",
        "# predict on a sample text without padding.\n",
        "\n",
        "sample_text = ('The movie was cool. The animation and the graphics '\n",
        "               'were out of this world. I would recommend this movie.')\n",
        "predictions = model.predict(np.array([sample_text]))\n",
        "print(predictions[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5ZI89aFHEC-",
        "outputId": "899b5d0b-eed6-4bab-d557-94d5eee8bc93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 361ms/step\n",
            "[0.01290772]\n"
          ]
        }
      ],
      "source": [
        "# step 4\n",
        "# predict on a sample text with padding\n",
        "\n",
        "padding = \"the \" * 2000\n",
        "predictions = model.predict(np.array([sample_text, padding]))\n",
        "print(predictions[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "c4u3PclZHL94"
      },
      "outputs": [],
      "source": [
        "# Compile the Keras model to configure the training process:\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCLskgScHRa-"
      },
      "source": [
        "##**Train Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "39xprOI5HUMj",
        "outputId": "c05d5f7b-aee1-48a2-a8aa-f90a943817ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "391/391 [==============================] - 655s 2s/step - loss: 0.6739 - accuracy: 0.5318 - val_loss: 0.6439 - val_accuracy: 0.5516\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 645s 2s/step - loss: 0.5359 - accuracy: 0.7381 - val_loss: 0.4731 - val_accuracy: 0.8021\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 624s 2s/step - loss: 0.4532 - accuracy: 0.8132 - val_loss: 0.4268 - val_accuracy: 0.8188\n",
            "Epoch 4/10\n",
            "196/391 [==============>...............] - ETA: 5:11 - loss: 0.4053 - accuracy: 0.8308"
          ]
        }
      ],
      "source": [
        "#step 1\n",
        "history = model.fit(train_dataset, epochs=10,\n",
        "                    validation_data=test_dataset,\n",
        "                    validation_steps=30)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtK+0UFyA7fqVgIaM00wOS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}